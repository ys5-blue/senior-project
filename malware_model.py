import torch
from torch import nn, optim
import torch.nn.functional as F
from pytorch_lightning import LightningModule

class MalwareModel(LightningModule):
    def __init__(self):
        super().__init__()
        self.embedding = nn.Embedding(256, 16)
        self.conv_params = [
            (16, 32, 16, 4),
            (32, 64, 16, 4),
            (64, 128, 16, 8),
            (128, 256, 8, 8),
            (256, 512, 8, 1),
        ]
        self.layers = nn.ModuleList([
            nn.Conv1d(*params[:3]) for params in self.conv_params
        ])
        self.logits = nn.Linear(512, 9)
        self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, x):
        x = self.embedding(x)
        x = torch.transpose(x, 1, -1)
        for layer, params in zip(self.layers, self.conv_params):
            _, _, kernel_size, pool_size = params
            if kernel_size > x.size(-1):
                break
            x = layer(x)
            x = F.relu(x)
            x = F.max_pool1d(x, pool_size)

        x = torch.mean(x, dim=-1)
        return self.logits(x)

    def configure_optimizers(self):
        return optim.Adam(self.parameters())

    def training_step(self, batch, batch_idx):
        features, labels = batch
        predicted_logits = self.forward(features)
        loss = self.loss_fn(predicted_logits, labels)
        accuracy = (predicted_logits.argmax(dim=-1) == labels).float().mean()
        self.log('acc', accuracy, prog_bar=True)
        self.log('loss', loss)
        return loss
